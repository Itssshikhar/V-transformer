{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798ccd0c-0ff0-45db-ab05-bd003f075a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0c701d-0b6e-40e1-9827-6478d1cf6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b848ad93-8e95-435c-b61f-c72c065178cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    #Convert the image into patches and then projection \n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c6a66d-8a4a-4726-bba2-47af7761bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    #Combine the patch embeddings with the class token and position embeddings.\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "\n",
    "        #making a token that can be added to input sequence and used to classify\n",
    "        self.classify_t = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
    "\n",
    "        #create position embeddings for the token and patch embeddings and adding 1 to sequence length for the token\n",
    "        self.position_embeddings = \\\n",
    "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
    "        self.Dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "\n",
    "        classify_ts = self.classify_token.expand(batch_size, -1, -1)\n",
    "\n",
    "        x = torch.cat((classify_t, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2913bd21-f5a6-461c-984e-fc1db4800515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    #single attention head\n",
    "    #multiple of these are used in multihead attention\n",
    "\n",
    "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "\n",
    "        #creating query, key and value projection layers\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #projecting the input in query, key and value\n",
    "        #then using the same to generate the query, value, and key\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        #attention scores\n",
    "        #softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_score = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.Dropout(attention_probs)\n",
    "\n",
    "        #calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        return (attenion_output, attention_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc103983-c034-4f71-a71f-a5095941476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    #multi head attention\n",
    "    #this module is used in Transformer encode module\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "\n",
    "        #calculation attention head size\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        #to use bias or not in projections\n",
    "        self.qkv_bias = config[\"bias\"]\n",
    "\n",
    "        #creating query, key and value projection layers\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422140f-6dc4-4489-b87b-f310fe9b65c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5dd9c8-83c0-4e40-95ca-3e9a9fb5ca97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af33e60-e428-444f-a867-009c3b5c865e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
