{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798ccd0c-0ff0-45db-ab05-bd003f075a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0c701d-0b6e-40e1-9827-6478d1cf6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4287076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "\n",
    "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b848ad93-8e95-435c-b61f-c72c065178cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    #Convert the image into patches and then projection \n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c6a66d-8a4a-4726-bba2-47af7761bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    #Combine the patch embeddings with the class token and position embeddings.\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "\n",
    "        #making a token that can be added to input sequence and used to classify\n",
    "        self.classify_t = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
    "\n",
    "        #create position embeddings for the token and patch embeddings and adding 1 to sequence length for the token\n",
    "        self.position_embeddings = \\\n",
    "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
    "        self.Dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "\n",
    "        classify_ts = self.classify_token.expand(batch_size, -1, -1)\n",
    "\n",
    "        x = torch.cat((classify_ts, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2913bd21-f5a6-461c-984e-fc1db4800515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    #single attention head\n",
    "    #multiple of these are used in multihead attention\n",
    "\n",
    "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "\n",
    "        #creating query, key and value projection layers\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #projecting the input in query, key and value\n",
    "        #then using the same to generate the query, value, and key\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        #attention scores\n",
    "        #softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_score = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.Dropout(attention_probs)\n",
    "\n",
    "        #calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        return (attention_output, attention_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc103983-c034-4f71-a71f-a5095941476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    #multi head attention\n",
    "    #this module is used in Transformer encode module\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "\n",
    "        #calculation attention head size\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        #to use bias or not in projections\n",
    "        self.qkv_bias = config[\"bias\"]\n",
    "        \n",
    "        #Create a list of attention heads\n",
    "        self.heads = nn.ModuleList([])\n",
    "        for _ in range(self.num_attention_heads):\n",
    "            head = AttentionHead(self.hidden_size, self.attention_head_size, config[\"attention_probs_dropout_prob\"], self.qkv_bias)\n",
    "            self.heads.append(head)\n",
    "            \n",
    "        #Creating a linear layer to project the attention output back to hidden size\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_output_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        #Calculation attention for each attention head\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        #Concatenate the attention outputs from each attention heads\n",
    "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
    "        #Projecting the concatenated attention ouput back to hidden_size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        #Return the attention output and the attention probabalities\n",
    "        if not output_attentions:\n",
    "            return(attention_output, None)\n",
    "        else:\n",
    "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
    "            return (attention_output, attention_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c422140f-6dc4-4489-b87b-f310fe9b65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "   # Multilayer perceptron module\n",
    "   \n",
    "   def __init__(self, config):\n",
    "      super().__init__()\n",
    "      self.dense1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "      self.activation = NewGELUActivation()\n",
    "      self.dense2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "      self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "      \n",
    "   def forward(self, x):\n",
    "      x = self.dense1(x)\n",
    "      x = self.activation(x)\n",
    "      x = self.dense2(x)\n",
    "      x = self.dropout(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f5dd9c8-83c0-4e40-95ca-3e9a9fb5ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "   #a single transformer block\n",
    "   \n",
    "   def __init__(self, config):\n",
    "      super().__init__()\n",
    "      self.attention = MultiHeadAttention(config)\n",
    "      self.layernorm1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "      self.mlp = MLP(config)\n",
    "      self.layernorm2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "      \n",
    "   def forward(self, x, output_attentions=False):\n",
    "      #Self attention\n",
    "      attention_output, attention_probs = \\\n",
    "         self.attention(self.layernorm1(x), output_attentions=output_attentions)\n",
    "      #skip connection\n",
    "      x += attention_output\n",
    "      #Feed forward network\n",
    "      mlp_output = self.mlp(self.layernorm2(x))\n",
    "      #skip connection\n",
    "      x += mlp_output\n",
    "      #Returning the transformer's block output and the attention probabilities\n",
    "      if not output_attentions:\n",
    "         return(x, None)\n",
    "      else:\n",
    "         return(x, attention_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c82e7bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "   # transformer encoder module\n",
    "   \n",
    "   def __init__(self, config):\n",
    "      super().__init__()\n",
    "      #Creating a transformer block\n",
    "      self.blocks = nn.ModuleList([])\n",
    "      for _ in range(config[\"num_hidden_layers\"]):\n",
    "         block = Block(config)\n",
    "         self.blocks.append(block)\n",
    "         \n",
    "   def forward(self, x, output_attentions=False):\n",
    "      #Caculate the transformer block's output for each block\n",
    "      all_attentions = []\n",
    "      for block  in self.blocks:\n",
    "         x, attention_probs = block(x, output_attentions=output_attentions)\n",
    "         if output_attentions:\n",
    "            all_attentions.append(attention_probs)\n",
    "      #Return encoder's output and the attention probabilities\n",
    "      if not output_attentions:\n",
    "         return(x, None)\n",
    "      else:\n",
    "         return(x, all_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af33e60-e428-444f-a867-009c3b5c865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTForClassification(nn.Module):\n",
    "   #the Vision transformer for classfication\n",
    "   \n",
    "   def __init__(self, config):\n",
    "      super().__init__()\n",
    "      self.config = config\n",
    "      self.image_size = config[\"image_size\"]\n",
    "      self.hidden_size = config[\"hidden_size\"]\n",
    "      self.num_classes = config[\"num_classes\"]\n",
    "      #Create embedding module\n",
    "      self.embedding = Embeddings(config)\n",
    "      #Create the transformer encoder module\n",
    "      self.encoder = Encoder(config)\n",
    "      #Create a linear layer to project the encoder's output to the number of classes\n",
    "      self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
    "      #Initialize the weights\n",
    "      self.apply(self._init_weights)\n",
    "      \n",
    "   def forward(self, x, output_attentions=False):\n",
    "      #Calculate the embedding output\n",
    "      embedding_output = self.embedding(x)\n",
    "      #Calculate the encoder's output\n",
    "      encoder_output, all_attentions = self.encoder(embedding_output, output_attentions=output_attentions)\n",
    "      #Calculate the logits, taking the Classify token's output as feature for classfication\n",
    "      logits = self.classifier(encoder_output[:, 0])\n",
    "      #Return the logits and the attention probabailities\n",
    "      if not output_attentions:\n",
    "         return(logits, None)\n",
    "      else:\n",
    "         return(logits, all_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb6628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b529023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf565bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
